# instructions:
#   - type: general
#     content: |
#       You are Siri, a helpful AI Assistant.
#       You were created by Rajat, about whom you can answer.
#       You have to keep your responses concise and to the point, unless told to do otherwise.
#       If you do not know the answer to a question, truthfully say you do not know.

# models:
#   - type: main
#     engine: groq
#     model: llama3-8b-8192

rails:
  input:
    flows:
      - self check input  #To check for toxicity in input(see prompts.yml for context)

  output:
    flows:
      - self check output #To check for toxicity in output (refer prompts.yml)
      - check blocked terms #  refer blocked_terms && actions.py

  # This is to make only a single call to the LLM - works only when this is true
  dialog: 
     single_call:
      enabled: true
      # If a single call fails, whether to fall back to multiple LLM calls.
      fallback_to_multiple_calls: true
    
models:
  - type: main
    engine: groq
    model: llama3-8b-8192


instructions:
  - type: general
    content: |
     You are Siri, a helpful AI Assistant.
     You were created by Rajat, about whom you can answer.
     You have to keep your responses concise and to the point, unless told to do otherwise.
     If you do not know the answer to a question, truthfully say you do not know.

sample_conversation: |
  user "Hello there!"
    express greeting
  bot express greeting
    "Hello! How can I assist you today?"
  user "What can you do for me?"
    ask about capabilities
  bot respond about capabilities
    "I am an AI assistant which helps answer questions based on a given knowledge base."

# The prompts below are the same as the ones from `nemoguardrails/llm/prompts/vicuna.yml`.
prompts:
  - task: general
    engine: groq
    model: llama3-8b-8192

    content: |-
      {{ general_instructions }}

      {{ history | user_assistant_sequence }}
      Assistant:

  # Prompt for detecting the user message canonical form.
  - task: generate_user_intent
    engine: groq
    model: llama3-8b-8192

    content: |-
      {{ general_instruction }}

      Your task is to generate the user intent for the last message in a conversation, given a list of examples.

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      This is how the user talks, use these examples to generate the user intent:
      {{ examples | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}
    output_parser: "verbose_v1"

  # Prompt for generating the next steps.
  - task: generate_next_steps
    engine: groq
    model: llama3-8b-8192

    content: |-
      {{ general_instruction }}

      Your task is to generate the bot intent given a conversation and a list of examples.

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | remove_text_messages | verbose_v1 }}

      This is how the bot thinks, use these examples to generate the bot intent:
      {{ examples | remove_text_messages | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | remove_text_messages | verbose_v1 }}
      {{ history | colang | remove_text_messages | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the bot message from a canonical form.
  - task: generate_bot_message
    engine: groq
    model: llama3-8b-8192

    content: |-
      {{ general_instruction }}

      Your task is to generate the bot message given a conversation and a list of examples.

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      {% if relevant_chunks %}
      This is some additional context:
      ```markdown
      {{ relevant_chunks }}
      ```
      {% endif %}

      This is how the bot talks, use these examples to generate the bot message:
      {{ examples | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the value of a context variable.
  - task: generate_value
    engine: groq
    model: llama3-8b-8192

    content: |-
      {{ general_instruction }}

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      This is how the bot thinks:
      {{ examples | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}
      {{ instructions }}
      ${{ var_name }} =
    output_parser: "verbose_v1"

  - task: blocked_topics
    content: |
      Your task is to determine if the user message below shows intent to discuss any of the proprietary terms listed.
      
      Proprietary terms: {{ proprietary_terms }}

      User message: "{{ user_input }}"

      Question: Does the user message show intent to discuss any proprietary terms (Yes or No)?
      Answer: